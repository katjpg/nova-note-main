# Cognitive Science Lecture Notes - Week 9
*November 2, 2024*
*Prof. Martinez - COGS 301*

## Neural Networks & Learning

Today's focus: How the brain learns and adapts through neural plasticity. Fascinating discussion on Hebbian learning - "neurons that fire together, wire together"

### Neural Architecture
```
Basic Neural Network Components:
Input Layer → Hidden Layer(s) → Output Layer
```

**Synaptic Plasticity:**
Long-term potentiation (LTP) strengthens connections between neurons through repeated activation. This forms the basis of learning at the cellular level.

> "The brain is like a muscle - the more you use it, the stronger it gets" - Prof. Martinez

---

## Learning Mechanisms

### Types of Learning:
- Supervised Learning
- Unsupervised Learning
- Reinforcement Learning

**Key Concept:** Backpropagation
How neural networks adjust connection weights to minimize error

*Missed 10 minutes of lecture - ask Sarah for notes*

### Computational Models

```python
# Simple perceptron example discussed in class
def perceptron(input_vector, weights):
    activation = sum(x*w for x,w in zip(input_vector, weights))
    return 1 if activation > 0 else 0
```

## Pattern Recognition

### Visual Processing Pipeline
1. Feature detection
2. Pattern assembly
3. Object recognition

**Important Distinction:**
Feed-forward vs Recurrent networks
- Feed-forward: One-way information flow
- Recurrent: Loops and feedback connections

---

## Clinical Applications

### Discussion Points:
- Neural plasticity in stroke recovery
- Learning disabilities and neural networks
- ??? (Check recording for this section)

**Research Examples:**
Deep learning applications in:
- Diagnosis
- Treatment planning
- Outcome prediction

---

## Class Demonstrations

### Today's Experiment:
Pattern recognition task showing:
1. Speed-accuracy trade-off
2. Learning curve effects
3. Individual differences in learning rates

*Remember to download data for homework*

---

## Questions Raised:

1. How do artificial neural networks differ from biological ones?
2. Role of emotion in learning?
3. Connection to previous week's attention models?

### For Further Investigation:
- Deep learning architectures
- Biological plausibility
- Scaling issues

---

## Next Week's Preview:
- Language Processing
- Semantic Networks
- Natural Language Processing

**Preparation:**
- Read Chapter 8
- Review Python basics for coding lab
- Watch recommended videos on deep learning

---

## Project Notes:

### Group Meeting Summary:
```
Timeline:
Week 10 - Data collection
Week 11 - Analysis
Week 12 - Presentation prep
```

*Remember to schedule simulation lab time*

---

### Personal Reminders:
- Download neural network simulator
- Practice implementing basic perceptron
- Review backpropagation math
- Email Prof about research opportunity

> "Complexity emerges from simple rules" - class discussion highlight

---

*End of lecture notes*

**Study Group Meeting:** Tomorrow, Library, 4PM
- Bring: Laptop, Week 8-9 notes
- Focus: Neural network implementations
